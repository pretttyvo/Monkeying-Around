{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Amazon, Helium 10, and Etsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "import urllib.request\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import urllib.request\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import locale\n",
    "from config import email, password\n",
    "# Import SQL Alchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Import and establish Base for which classes will be constructed \n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "\n",
    "# Import modules to declare columns and column data types\n",
    "from sqlalchemy import Column, Integer, String, Float\n",
    "from sqlalchemy.orm import Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class to pull data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class for scraping\n",
    "class amazon_h10():\n",
    "    def __init__(self, search):\n",
    "        print(f'You searched for {search}')\n",
    "    def complete_scrape(self, search):\n",
    "        print('you are scraping amazon')\n",
    "        amazon_url = \"https://www.amazon.com\"\n",
    "        # open the driver\n",
    "        options = Options()\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.implicitly_wait(30)\n",
    "        # scrape amazon      \n",
    "        search_df = pd.DataFrame()\n",
    "        search_df1 = pd.DataFrame()\n",
    "        amazon_df = pd.DataFrame()  \n",
    "        for word in search:\n",
    "            search_df1['search_term'] = [word]\n",
    "            search_df = search_df.append(search_df1)\n",
    "            try:\n",
    "                search_url = f'https://www.amazon.com/s?k={word}&ref=nb_sb_noss_1'\n",
    "                driver.get(search_url)\n",
    "                html = driver.page_source\n",
    "                search_beautify = BeautifulSoup(html, 'html.parser')\n",
    "                data = search_beautify.findAll('div', class_=\"a-section a-spacing-none\")\n",
    "                try:\n",
    "                    link = [link.a['href'] for link in data]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                data2 = search_beautify.findAll('span', class_=\"a-offscreen\")\n",
    "                time.sleep(3)\n",
    "                try:\n",
    "                    amz_prod_price = [x.text.split('$')[1] for x in data2]\n",
    "                except:\n",
    "                    pass\n",
    "                amz_prod_price = amz_prod_price[:10]\n",
    "                amz_prod_name = []\n",
    "\n",
    "                for i in range(12,22):\n",
    "                    product_link = amazon_url + link[i]\n",
    "                    break_link = product_link.split('/')\n",
    "                    title = break_link[3].replace('-', ' ')\n",
    "                    amz_prod_name.append(title)\n",
    "                amazon_df1 = pd.DataFrame()  \n",
    "                amazon_df1['product_name'] = amz_prod_name\n",
    "                amazon_df1['product_price'] = amz_prod_price\n",
    "                amazon_df1['search_term'] = [f'{word}' for count in amz_prod_name]\n",
    "                amazon_df = amazon_df.append(amazon_df1)\n",
    "            except:\n",
    "                pass\n",
    "#     ==================================\n",
    "        print('you are scraping helium')\n",
    "        # sign into Helium 10\n",
    "        driver.get('https://members.helium10.com/user/signin')\n",
    "        login = driver.find_element_by_id('loginform-email')\n",
    "        login.send_keys(email)\n",
    "        password_route = driver.find_element_by_id('loginform-password')\n",
    "        password_route.send_keys(password)\n",
    "        login_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Log In')]\")\n",
    "        login_button[0].click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # create global data frame\n",
    "        overview_data_df = pd.DataFrame()\n",
    "        top5_monthly_revenue_df = pd.DataFrame()\n",
    "        overview_data_df1 = pd.DataFrame()\n",
    "        top5_monthly_revenue_df1 = pd.DataFrame()\n",
    "        \n",
    "        # type in the key words search\n",
    "        for words in search:\n",
    "            #  link for niche\n",
    "            h_10_niche = \"https://members.helium10.com/black-box/niche\"\n",
    "            # visit link\n",
    "            driver.get(h_10_niche)\n",
    "            search_bar = driver.find_element_by_id('filter-asin')\n",
    "            search_bar.clear()\n",
    "            search_bar.send_keys(words)\n",
    "            \n",
    "            try:\n",
    "                # click on search button\n",
    "                time.sleep(1)\n",
    "                search_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Search')]\")\n",
    "                search_button[0].click()\n",
    "            except:\n",
    "                time.sleep(2)\n",
    "                search_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Search')]\")\n",
    "                search_button[0].click()\n",
    "\n",
    "            # Scrape overview data    \n",
    "            try: \n",
    "                overview_data = driver.find_elements_by_xpath(\"//div[@class='col-xs-12 col-sm-6 col-md-2']\")\n",
    "\n",
    "                col1 = []\n",
    "                col2 = []\n",
    "                time.sleep(1)\n",
    "                for i in range(0,5):\n",
    "                    h_10_data = overview_data[i].text\n",
    "                    h_data = h_10_data.split('\\n')\n",
    "                    col1.append(h_data[0].strip())\n",
    "                    value = h_data[1].split('$')\n",
    "                    if len(value) > 1:\n",
    "                        value = value[1]\n",
    "                        col2.append(value)\n",
    "                    else:\n",
    "                        col2.append(value[0])\n",
    "                # pull data from 20-24\n",
    "                overview_data_df1['revenue_specs'] = col1\n",
    "                overview_data_df1['revenue_value'] = col2\n",
    "                overview_data_df1['search_term'] = [f'{words}' for x in col1]\n",
    "                overview_data_df = overview_data_df.append(overview_data_df1)\n",
    "\n",
    "                # Pull revenue data  \n",
    "                sort_product_list = Select(driver.find_element_by_id('sort'))\n",
    "                driver.find_element_by_xpath(\"//select[@name='sort']/option[text()='Monthly Revenue High To Low']\").click()\n",
    "\n",
    "                # pull in data\n",
    "                # product title\n",
    "                time.sleep(1)\n",
    "                product_name = driver.find_elements_by_class_name('media-heading')\n",
    "                product_names = [name.text for name in product_name]\n",
    "\n",
    "                # price\n",
    "                product_price = driver.find_elements_by_class_name('price-chart')\n",
    "                product_prices = [price.text.split('$')[1] for price in product_price]\n",
    "\n",
    "                # monthly sales\n",
    "                product_sales_monthly = driver.find_elements_by_class_name('monthlySales-column')\n",
    "                product_sales_monthly_s = [sales.text for sales in product_sales_monthly]\n",
    "\n",
    "                # monthly revenue \n",
    "                product_revenue_monthly = driver.find_elements_by_class_name('monthlyRevenue-column')\n",
    "                product_revenue_monthly_s = [rev.text.split('$')[1] for rev in product_revenue_monthly]\n",
    "\n",
    "                # put into a df\n",
    "                top5_monthly_revenue_df1['monthly_product'] = product_names[:5]\n",
    "                top5_monthly_revenue_df1['monthly_price'] = product_prices[:5]\n",
    "                top5_monthly_revenue_df1['monthly_sales'] = product_sales_monthly_s[:5]\n",
    "                top5_monthly_revenue_df1['monthly_revenue'] = product_revenue_monthly_s[:5]\n",
    "                top5_monthly_revenue_df1['search_term'] = [f'{words}' for x in product_names[:5]]\n",
    "                top5_monthly_revenue_df = top5_monthly_revenue_df.append(top5_monthly_revenue_df1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "#     ====================\n",
    "        etsy_df = pd.DataFrame()\n",
    "        etsy_df1 = pd.DataFrame()\n",
    "        print(\"you are scraping etsy\")\n",
    "        # loop over search words        \n",
    "        for word in search:\n",
    "            # visit url\n",
    "            search_url = f'https://www.etsy.com/search?q={word}&explicit=1&order=most_relevant'\n",
    "            driver.get(search_url)\n",
    "            \n",
    "            # product name\n",
    "            try:\n",
    "                product = driver.find_elements_by_class_name(\"v2-listing-card__info\")\n",
    "                product_name = [link.text.strip() for link in product]\n",
    "\n",
    "                # produce price\n",
    "                price = driver.find_elements_by_class_name(\"n-listing-card__price\")\n",
    "                product_price = [x.text.strip() for x in price]\n",
    "                # clean data \n",
    "                n = [x.split('\\n') for x in product_price]\n",
    "                product_price = [y[0] for y in n]\n",
    "                b = [x.split('$') for x in product_price]\n",
    "                locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' ) \n",
    "                try:\n",
    "                    product_price = [locale.atof(y[1]) for y in b]\n",
    "                except:\n",
    "                    product_price = [y[1] for y in b]\n",
    "\n",
    "                # df to hold the data    \n",
    "                etsy_df1['product_name'] = product_name[:5]\n",
    "                etsy_df1['product_price'] = product_price[:5]\n",
    "                etsy_df1['search_term'] = [f'{word}' for x in product_name[:5]]\n",
    "                etsy_df = etsy_df.append(etsy_df1)\n",
    "            except:\n",
    "                pass\n",
    "        driver.close()\n",
    "        print(\"scraping complete\")\n",
    "        search_df = search_df.set_index(['search_term'])\n",
    "        amazon_df = amazon_df.set_index(['search_term'])\n",
    "        overview_data_df = overview_data_df.set_index(['search_term'])\n",
    "        top5_monthly_revenue_df = top5_monthly_revenue_df.set_index(['search_term'])\n",
    "        etsy_df = etsy_df.set_index(['search_term'])\n",
    "        return [search_df, amazon_df, overview_data_df, top5_monthly_revenue_df, etsy_df]\n",
    "\n",
    "    def amazon(self, search):\n",
    "        print('you are scraping amazon')\n",
    "        amazon_url = \"https://www.amazon.com\"\n",
    "        # open the driver\n",
    "        options = Options()\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.implicitly_wait(30)\n",
    "        # scrape amazon      \n",
    "        search_df = pd.DataFrame()\n",
    "        search_df1 = pd.DataFrame()\n",
    "        amazon_df = pd.DataFrame()  \n",
    "        for word in search:\n",
    "            search_df1['search_term'] = [word]\n",
    "            search_df = search_df.append(search_df1)\n",
    "            try:\n",
    "                search_url = f'https://www.amazon.com/s?k={word}&ref=nb_sb_noss_1'\n",
    "                driver.get(search_url)\n",
    "                html = driver.page_source\n",
    "                search_beautify = BeautifulSoup(html, 'html.parser')\n",
    "                data = search_beautify.findAll('div', class_=\"a-section a-spacing-none\")\n",
    "                try:\n",
    "                    link = [link.a['href'] for link in data]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                data2 = search_beautify.findAll('span', class_=\"a-offscreen\")\n",
    "                time.sleep(3)\n",
    "                try:\n",
    "                    amz_prod_price = [x.text.split('$')[1] for x in data2]\n",
    "                except:\n",
    "                    pass\n",
    "                amz_prod_price = amz_prod_price[:10]\n",
    "                amz_prod_name = []\n",
    "\n",
    "                for i in range(12,22):\n",
    "                    product_link = amazon_url + link[i]\n",
    "                    break_link = product_link.split('/')\n",
    "                    title = break_link[3].replace('-', ' ')\n",
    "                    amz_prod_name.append(title)\n",
    "                amazon_df1 = pd.DataFrame()  \n",
    "                amazon_df1['product_name'] = amz_prod_name\n",
    "                amazon_df1['product_price'] = amz_prod_price\n",
    "                amazon_df1['search_term'] = [f'{word}' for count in amz_prod_name]\n",
    "                amazon_df = amazon_df.append(amazon_df1)\n",
    "            except:\n",
    "                pass\n",
    "        driver.close()\n",
    "        amazon_df = amazon_df.set_index(['search_term'])\n",
    "        return amazon_df\n",
    "        \n",
    "    def helium(self, search):\n",
    "        print('you are scraping helium')\n",
    "        # sign into Helium 10\n",
    "        driver.get('https://members.helium10.com/user/signin')\n",
    "        login = driver.find_element_by_id('loginform-email')\n",
    "        login.send_keys(email)\n",
    "        password_route = driver.find_element_by_id('loginform-password')\n",
    "        password_route.send_keys(password)\n",
    "        login_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Log In')]\")\n",
    "        login_button[0].click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # create global data frame\n",
    "        overview_data_df = pd.DataFrame()\n",
    "        top5_monthly_revenue_df = pd.DataFrame()\n",
    "        overview_data_df1 = pd.DataFrame()\n",
    "        top5_monthly_revenue_df1 = pd.DataFrame()\n",
    "        \n",
    "        # type in the key words search\n",
    "        for words in search:\n",
    "            #  link for niche\n",
    "            h_10_niche = \"https://members.helium10.com/black-box/niche\"\n",
    "            # visit link\n",
    "            driver.get(h_10_niche)\n",
    "            search_bar = driver.find_element_by_id('filter-asin')\n",
    "            search_bar.clear()\n",
    "            search_bar.send_keys(words)\n",
    "            \n",
    "            try:\n",
    "                # click on search button\n",
    "                time.sleep(1)\n",
    "                search_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Search')]\")\n",
    "                search_button[0].click()\n",
    "            except:\n",
    "                time.sleep(2)\n",
    "                search_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Search')]\")\n",
    "                search_button[0].click()\n",
    "\n",
    "            # Scrape overview data    \n",
    "            try: \n",
    "                overview_data = driver.find_elements_by_xpath(\"//div[@class='col-xs-12 col-sm-6 col-md-2']\")\n",
    "\n",
    "                col1 = []\n",
    "                col2 = []\n",
    "                time.sleep(1)\n",
    "                for i in range(0,5):\n",
    "                    h_10_data = overview_data[i].text\n",
    "                    h_data = h_10_data.split('\\n')\n",
    "                    col1.append(h_data[0].strip())\n",
    "                    value = h_data[1].split('$')\n",
    "                    if len(value) > 1:\n",
    "                        value = value[1]\n",
    "                        col2.append(value)\n",
    "                    else:\n",
    "                        col2.append(value[0])\n",
    "                # pull data from 20-24\n",
    "                overview_data_df1['revenue_specs'] = col1\n",
    "                overview_data_df1['revenue_value'] = col2\n",
    "                overview_data_df1['search_term'] = [f'{words}' for x in col1]\n",
    "                overview_data_df = overview_data_df.append(overview_data_df1)\n",
    "\n",
    "                # Pull revenue data  \n",
    "                sort_product_list = Select(driver.find_element_by_id('sort'))\n",
    "                driver.find_element_by_xpath(\"//select[@name='sort']/option[text()='Monthly Revenue High To Low']\").click()\n",
    "\n",
    "                # pull in data\n",
    "                # product title\n",
    "                time.sleep(1)\n",
    "                product_name = driver.find_elements_by_class_name('media-heading')\n",
    "                product_names = [name.text for name in product_name]\n",
    "\n",
    "                # price\n",
    "                product_price = driver.find_elements_by_class_name('price-chart')\n",
    "                product_prices = [price.text.split('$')[1] for price in product_price]\n",
    "\n",
    "                # monthly sales\n",
    "                product_sales_monthly = driver.find_elements_by_class_name('monthlySales-column')\n",
    "                product_sales_monthly_s = [sales.text for sales in product_sales_monthly]\n",
    "\n",
    "                # monthly revenue \n",
    "                product_revenue_monthly = driver.find_elements_by_class_name('monthlyRevenue-column')\n",
    "                product_revenue_monthly_s = [rev.text.split('$')[1] for rev in product_revenue_monthly]\n",
    "\n",
    "                # put into a df\n",
    "                top5_monthly_revenue_df1['monthly_product'] = product_names[:5]\n",
    "                top5_monthly_revenue_df1['monthly_price'] = product_prices[:5]\n",
    "                top5_monthly_revenue_df1['monthly_sales'] = product_sales_monthly_s[:5]\n",
    "                top5_monthly_revenue_df1['monthly_revenue'] = product_revenue_monthly_s[:5]\n",
    "                top5_monthly_revenue_df1['search_term'] = [f'{words}' for x in product_names[:5]]\n",
    "                top5_monthly_revenue_df = top5_monthly_revenue_df.append(top5_monthly_revenue_df1)\n",
    "            except:\n",
    "                pass\n",
    "        driver.close()\n",
    "        overview_data_df = overview_data_df.set_index(['search_term'])\n",
    "        top5_monthly_revenue_df = top5_monthly_revenue_df.set_index(['search_term'])\n",
    "        return [overview_data_df, top5_monthly_revenue_df]\n",
    "    def etsy(self, search):\n",
    "        options = Options()\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.implicitly_wait(30)\n",
    "        etsy_df = pd.DataFrame()\n",
    "        etsy_df1 = pd.DataFrame()\n",
    "        print(\"you are scraping etsy\")\n",
    "        # loop over search words        \n",
    "        for word in search:\n",
    "            # visit url\n",
    "            search_url = f'https://www.etsy.com/search?q={word}&explicit=1&order=most_relevant'\n",
    "            driver.get(search_url)\n",
    "            \n",
    "            # product name\n",
    "            try:\n",
    "                product = driver.find_elements_by_class_name(\"v2-listing-card__info\")\n",
    "                product_name = [link.text.strip() for link in product]\n",
    "\n",
    "                # produce price\n",
    "                price = driver.find_elements_by_class_name(\"n-listing-card__price\")\n",
    "                product_price = [x.text.strip() for x in price]\n",
    "                # clean data \n",
    "                n = [x.split('\\n') for x in product_price]\n",
    "                product_price = [y[0] for y in n]\n",
    "                b = [x.split('$') for x in product_price]\n",
    "                locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' ) \n",
    "                try:\n",
    "                    product_price = [locale.atof(y[1]) for y in b]\n",
    "                except:\n",
    "                    product_price = [y[1] for y in b]\n",
    "\n",
    "                # df to hold the data    \n",
    "                etsy_df1['product_name'] = product_name[:5]\n",
    "                etsy_df1['product_price'] = product_price[:5]\n",
    "                etsy_df1['search_term'] = [f'{word}' for x in product_name[:5]]\n",
    "                etsy_df = etsy_df.append(etsy_df1)\n",
    "            except:\n",
    "                pass\n",
    "        driver.close()\n",
    "        print(\"scraping complete\")\n",
    "        etsy_df = etsy_df.set_index(['search_term'])\n",
    "        driver.close()\n",
    "        return [etsy_df]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  original\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pymongo\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "import urllib.request\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import urllib.request\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import locale\n",
    "\n",
    "class amazon_h10():\n",
    "    def __init__(self, search):\n",
    "        print(f'You searched for {search}')\n",
    "    def complete_scrape(self, search):\n",
    "        print('you are scraping amazon')\n",
    "        amazon_url = \"https://www.amazon.com\"\n",
    "        # open the driver\n",
    "        options = Options()\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.implicitly_wait(30)\n",
    "        # scrape amazon      \n",
    "        amazon_df = pd.DataFrame()  \n",
    "        for word in search:\n",
    "            \n",
    "            try:\n",
    "                search_url = f'https://www.amazon.com/s?k={word}&ref=nb_sb_noss_1'\n",
    "                driver.get(search_url)\n",
    "                html = driver.page_source\n",
    "                search_beautify = BeautifulSoup(html, 'html.parser')\n",
    "                data = search_beautify.findAll('div', class_=\"a-section a-spacing-none\")\n",
    "                try:\n",
    "                    link = [link.a['href'] for link in data]\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                data2 = search_beautify.findAll('span', class_=\"a-offscreen\")\n",
    "                time.sleep(3)\n",
    "                try:\n",
    "                    amz_prod_price = [x.text.split('$')[1] for x in data2]\n",
    "                except:\n",
    "                    pass\n",
    "                amz_prod_price = amz_prod_price[:10]\n",
    "                amz_prod_name = []\n",
    "\n",
    "                for i in range(12,22):\n",
    "                    product_link = amazon_url + link[i]\n",
    "                    break_link = product_link.split('/')\n",
    "                    title = break_link[3].replace('-', ' ')\n",
    "                    amz_prod_name.append(title)\n",
    "                amazon_df1 = pd.DataFrame()  \n",
    "                amazon_df1['product_name'] = amz_prod_name\n",
    "                amazon_df1['product_price'] = amz_prod_price\n",
    "                amazon_df1['search_term'] = [f'{word}' for count in amz_prod_name]\n",
    "                amazon_df = amazon_df.append(amazon_df1)\n",
    "            except:\n",
    "                pass\n",
    "#     ==================================\n",
    "        print('you are scraping helium')\n",
    "        # sign into Helium 10\n",
    "        driver.get('https://members.helium10.com/user/signin')\n",
    "        login = driver.find_element_by_id('loginform-email')\n",
    "        login.send_keys(email)\n",
    "        password_route = driver.find_element_by_id('loginform-password')\n",
    "        password_route.send_keys(password)\n",
    "        login_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Log In')]\")\n",
    "        login_button[0].click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # create global data frame\n",
    "        overview_data_df = pd.DataFrame()\n",
    "        top5_monthly_revenue_df = pd.DataFrame()\n",
    "\n",
    "        # type in the key words search\n",
    "        for words in search:\n",
    "            #  link for niche\n",
    "            h_10_niche = \"https://members.helium10.com/black-box/niche\"\n",
    "            # visit link\n",
    "            driver.get(h_10_niche)\n",
    "            search_bar = driver.find_element_by_id('filter-asin')\n",
    "            search_bar.clear()\n",
    "            search_bar.send_keys(words)\n",
    "            \n",
    "            # click on search button\n",
    "            search_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Search')]\")\n",
    "            search_button[0].click()\n",
    "\n",
    "            # Scrape overview data    \n",
    "            try: \n",
    "                overview_data = driver.find_elements_by_xpath(\"//div[@class='col-xs-12 col-sm-6 col-md-2']\")\n",
    "\n",
    "                col1 = []\n",
    "                col2 = []\n",
    "                time.sleep(1)\n",
    "                for i in range(0,5):\n",
    "                    h_10_data = overview_data[i].text\n",
    "                    h_data = h_10_data.split('\\n')\n",
    "                    col1.append(h_data[0].strip())\n",
    "                    value = h_data[1].split('$')\n",
    "                    if len(value) > 1:\n",
    "                        value = value[1]\n",
    "                        col2.append(value)\n",
    "                    else:\n",
    "                        col2.append(value[0])\n",
    "                # pull data from 20-24\n",
    "                overview_data_df['revenue_specs'] = col1\n",
    "                overview_data_df['revenue_value'] = col2\n",
    "                overview_data_df['search_term'] = [f'{word}' for count in col1]\n",
    "\n",
    "                # Pull revenue data  \n",
    "                sort_product_list = Select(driver.find_element_by_id('sort'))\n",
    "                driver.find_element_by_xpath(\"//select[@name='sort']/option[text()='Monthly Revenue High To Low']\").click()\n",
    "\n",
    "                # pull in data\n",
    "                # product title\n",
    "                time.sleep(1)\n",
    "                product_name = driver.find_elements_by_class_name('media-heading')\n",
    "                product_names = [name.text for name in product_name]\n",
    "\n",
    "                # price\n",
    "                product_price = driver.find_elements_by_class_name('price-chart')\n",
    "                product_prices = [price.text.split('$')[1] for price in product_price]\n",
    "\n",
    "                # monthly sales\n",
    "                product_sales_monthly = driver.find_elements_by_class_name('monthlySales-column')\n",
    "                product_sales_monthly_s = [sales.text for sales in product_sales_monthly]\n",
    "\n",
    "                # monthly revenue \n",
    "                product_revenue_monthly = driver.find_elements_by_class_name('monthlyRevenue-column')\n",
    "                product_revenue_monthly_s = [rev.text.split('$')[1] for rev in product_revenue_monthly]\n",
    "\n",
    "                # put into a df\n",
    "                top5_monthly_revenue_df['monthly_product'] = product_names[:5]\n",
    "                top5_monthly_revenue_df['monthly_price'] = product_prices[:5]\n",
    "                top5_monthly_revenue_df['monthly_sales'] = product_sales_monthly_s[:5]\n",
    "                top5_monthly_revenue_df['monthly_revenue'] = product_revenue_monthly_s[:5]\n",
    "                top5_monthly_revenue_df['search_term'] = [f'{word}' for count in product_names[:5]]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "#     ====================\n",
    "        etsy_df = pd.DataFrame()\n",
    "        print(\"you are scraping etsy\")\n",
    "        # loop over search words        \n",
    "        for word in search:\n",
    "            # visit url\n",
    "            search_url = f'https://www.etsy.com/search?q={word}&explicit=1&order=most_relevant'\n",
    "            driver.get(search_url)\n",
    "            \n",
    "            # product name\n",
    "            try:\n",
    "                product = driver.find_elements_by_class_name(\"v2-listing-card__info\")\n",
    "                product_name = [link.text.strip() for link in product]\n",
    "\n",
    "                # produce price\n",
    "                price = driver.find_elements_by_class_name(\"n-listing-card__price\")\n",
    "                product_price = [x.text.strip() for x in price]\n",
    "                # clean data \n",
    "                n = [x.split('\\n') for x in product_price]\n",
    "                product_price = [y[0] for y in n]\n",
    "                b = [x.split('$') for x in product_price]\n",
    "                locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' ) \n",
    "                try:\n",
    "                    product_price = [locale.atof(y[1]) for y in b]\n",
    "                except:\n",
    "                    product_price = [y[1] for y in b]\n",
    "\n",
    "                # df to hold the data    \n",
    "                etsy_df['product_name'] = product_name[:5]\n",
    "                etsy_df['product_price'] = product_price[:5]\n",
    "                etsy_df['search_term'] = [f'{word}' for count in product_name[:5]]\n",
    "            except:\n",
    "                pass\n",
    "        driver.close()\n",
    "        print(\"scraping complete\")\n",
    "        return [amazon_df, overview_data_df, top5_monthly_revenue_df, etsy_df]\n",
    "    def amazon(self, search):\n",
    "        print('you are scraping amazon')\n",
    "                # open the driver\n",
    "        # unpacked_extension_path = '/Users/prettyvo/Library/Application Support/Google/Chrome/Default/Extensions/njmehopjdpcckochcggncklnlmikcbnb/4.2_0'\n",
    "        options = Options()\n",
    "        # options.add_argument('--load-extension={}'.format(unpacked_extension_path))\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.implicitly_wait(30)\n",
    "        # scrape amazon \n",
    "                # open up amazon        \n",
    "        amazon_url = \"https://www.amazon.com\"\n",
    "        driver.get(amazon_url)\n",
    "        amazon_df = pd.DataFrame()   \n",
    "        for word in search:\n",
    "            search_url = f'https://www.amazon.com/s?k={word}&ref=nb_sb_noss_1'\n",
    "            driver.get(search_url)\n",
    "            # time.sleep(3)\n",
    "            html = driver.page_source\n",
    "            search_beautify = BeautifulSoup(html, 'html.parser')\n",
    "            data = search_beautify.findAll('div', class_=\"a-section a-spacing-none\")\n",
    "            try:\n",
    "                link = [link.a['href'] for link in data]\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            data2 = search_beautify.findAll('span', class_=\"a-offscreen\")\n",
    "            time.sleep(3)\n",
    "            try:\n",
    "                amz_prod_price = [x.text.split('$')[1] for x in data2]\n",
    "            except:\n",
    "                pass\n",
    "            amz_prod_price = amz_prod_price[:10]\n",
    "            amz_prod_name = []\n",
    "            amz_prod_asin = []\n",
    "\n",
    "            for i in range(12,22):\n",
    "                product_link = amazon_url + link[i]\n",
    "                break_link = product_link.split('/')\n",
    "                title = break_link[3].replace('-', ' ')\n",
    "                amz_prod_name.append(title)\n",
    "                amz_prod_asin.append(break_link[5])\n",
    "\n",
    "            amazon_df[f'{word}_name'] = amz_prod_name   \n",
    "            amazon_df[f'{word}_price'] = amz_prod_price\n",
    "            amazon_df[f'{word}_asin'] = amz_prod_asin\n",
    "        driver.close()\n",
    "        return amazon_df\n",
    "        \n",
    "    def helium(self, search):\n",
    "        print('you are scraping helium')\n",
    "        # open the driver\n",
    "        # unpacked_extension_path = '/Users/prettyvo/Library/Application Support/Google/Chrome/Default/Extensions/njmehopjdpcckochcggncklnlmikcbnb/4.2_0'\n",
    "        options = Options()\n",
    "        # options.add_argument('--load-extension={}'.format(unpacked_extension_path))\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.implicitly_wait(30)\n",
    "\n",
    "        # sign into Helium 10\n",
    "        driver.get('https://members.helium10.com/user/signin')\n",
    "        login = driver.find_element_by_id('loginform-email')\n",
    "        login.send_keys('justin@breathingcolor.com')\n",
    "        password = driver.find_element_by_id('loginform-password')\n",
    "        password.send_keys('TjDxAfM0w2m')\n",
    "        login_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Log In')]\")\n",
    "        login_button[0].click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # #  link for niche\n",
    "        # h_10_niche = \"https://members.helium10.com/black-box/niche\"\n",
    "        # # visit link\n",
    "        # driver.get(h_10_niche)\n",
    "        # time.sleep(2)\n",
    "        \n",
    "        # create global data frame\n",
    "        overview_data_df = pd.DataFrame()\n",
    "        top5_monthly_revenue_df = pd.DataFrame()\n",
    "        \n",
    "        # # select category\n",
    "        # time.sleep(2)\n",
    "        # categories = driver.find_elements_by_class_name(\"dropdown-toggle\")\n",
    "        # categories[0].click()\n",
    "        \n",
    "        \n",
    "        # # Get catofories names\n",
    "        # categories_list = driver.find_elements_by_class_name(\"checkbox\")\n",
    "        # categories_list_names = [x.text for x in categories_list]\n",
    "        # cleaned_categories = list(filter(None, categories_list_names))\n",
    "        # # categories_df = pd.DataFrame(cleaned_categories, columns=['categories'])\n",
    "        \n",
    "        # type in the key words search\n",
    "        for words in search:\n",
    "            #  link for niche\n",
    "            h_10_niche = \"https://members.helium10.com/black-box/niche\"\n",
    "            # visit link\n",
    "            driver.get(h_10_niche)\n",
    "            search_bar = driver.find_element_by_id('filter-asin')\n",
    "            search_bar.clear()\n",
    "            search_bar.send_keys(words)\n",
    "            # open categories list\n",
    "            # categories = driver.find_elements_by_class_name(\"dropdown-toggle\")\n",
    "            # categories[0].click()\n",
    "            # time.sleep(1)\n",
    "\n",
    "    # ----------------remove----------------\n",
    "            # restrict = 'Select all'\n",
    "\n",
    "            # for i in range(0,24):\n",
    "            #     if restrict == categories_list_names[i]:\n",
    "            #         categories_list = driver.find_elements_by_class_name(\"checkbox\")\n",
    "            #         categories_list[i].click()\n",
    "            #     else:\n",
    "            #         pass\n",
    "            \n",
    "            # # close out the toggle\n",
    "            # categories = driver.find_elements_by_class_name(\"dropdown-toggle\")\n",
    "            # categories[0].click()\n",
    "            # click on search button\n",
    "            search_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Search')]\")\n",
    "            search_button[0].click()\n",
    "            search_button = driver.find_elements_by_xpath(\"//button[contains(text(), 'Search')]\")\n",
    "            search_button[0].click()\n",
    "\n",
    "            # Scrape overview data    \n",
    "\n",
    "            overview_data = driver.find_elements_by_xpath(\"//div[@class='col-xs-12 col-sm-6 col-md-2']\")\n",
    "            print(len(overview_data))\n",
    "            \n",
    "            col1 = []\n",
    "            col2 = []\n",
    "            time.sleep(1)\n",
    "            for i in range(0,5):\n",
    "                h_10_data = overview_data[i].text\n",
    "                h_data = h_10_data.split('\\n')\n",
    "                col1.append(h_data[0].strip())\n",
    "                value = h_data[1].split('$')\n",
    "                if len(value) > 1:\n",
    "                    value = value[1]\n",
    "                    col2.append(value)\n",
    "                else:\n",
    "                    col2.append(value[0])\n",
    "            # pull data from 20-24\n",
    "            overview_data_df[f'{words}_specs'] = col1\n",
    "            overview_data_df[f'{words}_value'] = col2\n",
    "\n",
    "            # Pull revenue data  \n",
    "            sort_product_list = Select(driver.find_element_by_id('sort'))\n",
    "            driver.find_element_by_xpath(\"//select[@name='sort']/option[text()='Monthly Revenue High To Low']\").click()\n",
    "\n",
    "            # pull in data\n",
    "            # product title\n",
    "            time.sleep(1)\n",
    "            product_name = driver.find_elements_by_class_name('media-heading')\n",
    "            product_names = [name.text for name in product_name]\n",
    "\n",
    "            # price\n",
    "            product_price = driver.find_elements_by_class_name('price-chart')\n",
    "            product_prices = [price.text.split('$')[1] for price in product_price]\n",
    "\n",
    "            # monthly sales\n",
    "            product_sales_monthly = driver.find_elements_by_class_name('monthlySales-column')\n",
    "            product_sales_monthly_s = [sales.text for sales in product_sales_monthly]\n",
    "            print(len(product_sales_monthly_s))\n",
    "\n",
    "            # monthly revenue \n",
    "            product_revenue_monthly = driver.find_elements_by_class_name('monthlyRevenue-column')\n",
    "            product_revenue_monthly_s = [rev.text.split('$')[1] for rev in product_revenue_monthly]\n",
    "\n",
    "            # put into a df\n",
    "            top5_monthly_revenue_df[f'{words}_product'] = product_names[:5]\n",
    "            top5_monthly_revenue_df[f'{words}_price'] = product_prices[:5]\n",
    "            top5_monthly_revenue_df[f'{words}_monthly_sales'] = product_sales_monthly_s[:5]\n",
    "            top5_monthly_revenue_df[f'{words}_monthly_revenue'] = product_revenue_monthly_s[:5]\n",
    "        driver.close()\n",
    "        return [overview_data_df, top5_monthly_revenue_df]\n",
    "    def etsy(self, search):\n",
    "        etsy_df = pd.DataFrame()\n",
    "        for word in search:\n",
    "            # launch chrome driver\n",
    "            executable_path = {'executable_path': '/Users/prettyvo/Desktop/chromedriver'}\n",
    "            browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "            base_url = f'https://www.etsy.com'\n",
    "            browser.visit(base_url)\n",
    "            html = browser.html\n",
    "            etsy_html = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            search_url = f'https://www.etsy.com/search?q={word}&explicit=1&order=most_relevant'\n",
    "            browser.visit(search_url)\n",
    "            visit_html = browser.html\n",
    "            word_html = BeautifulSoup(visit_html, 'html.parser')\n",
    "\n",
    "            # product name\n",
    "            product = word_html.findAll('div', class_=\"v2-listing-card__info\")\n",
    "            product_name = [link.h2.text.strip() for link in product]\n",
    "\n",
    "            # produce price\n",
    "            price = word_html.findAll('span', class_=\"n-listing-card__price\")\n",
    "            product_price = [x.text.strip() for x in price]\n",
    "            # clean data \n",
    "            n = [x.split('\\n') for x in product_price]\n",
    "            product_price = [y[0] for y in n]\n",
    "            b = [x.split('$') for x in product_price]\n",
    "            locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' ) \n",
    "            try:\n",
    "                product_price = [float(y[1]) for y in b]\n",
    "            except:\n",
    "                product_price = [locale.atof(y[1]) for y in b]\n",
    "            # print(type(product_price[0]))\n",
    "\n",
    "            # pull in product rating\n",
    "            rating = word_html.findAll('div', class_=\"v2-listing-card__shop\")\n",
    "            r_score = [x.span.input for x in rating]\n",
    "            scores = []\n",
    "            for x in r_score:\n",
    "                try:\n",
    "                    scores.append(float(x['value']))\n",
    "                except:\n",
    "                    scores.append(float('0'))\n",
    "                    \n",
    "            # df to hold the data    \n",
    "            etsy_df[f'{word}_name'] = product_name\n",
    "            etsy_df[f'{word}_price'] = product_price\n",
    "            etsy_df[f'{word}_rating'] = scores\n",
    "        browser.quit()\n",
    "        return etsy_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call class to scrape data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example input data from user\n",
    "keywords = ['candy', 'chocolate','lollipop','bananas','oranges','apples','tacos','kebabs','sandwiches']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pen']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0, (len(search_words)-1)):\n",
    "    for j in range(0, (len(searched)-1)):\n",
    "        if search_words[i] == searched[j]:\n",
    "            del search_words[i]\n",
    "        else:\n",
    "            pass\n",
    "search_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_terms = keywords\n",
    "old_terms = []\n",
    "# check to see if data is already in database\n",
    "for i in range(0, (len(keywords)-1)):\n",
    "    for j in range(0, (len(searched)-1)):\n",
    "        if keywords[i] == searched[j]:\n",
    "            old_terms.append(keywords[i])\n",
    "            del new_terms[i]\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candy']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in search_words:\n",
    "    searched.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['candy',\n",
       " 'chocolate',\n",
       " 'lollipop',\n",
       " 'bananas',\n",
       " 'oranges',\n",
       " 'apples',\n",
       " 'tacos',\n",
       " 'kebabs',\n",
       " 'sandwiches']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call class and scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You searched for ['candy', 'chocolate', 'lollipop', 'bananas', 'oranges', 'apples', 'tacos', 'kebabs', 'sandwiches']\n",
      "you are scraping amazon\n",
      "you are scraping helium\n",
      "you are scraping etsy\n",
      "scraping complete\n"
     ]
    }
   ],
   "source": [
    "# call class\n",
    "class_testing = amazon_h10(keywords)\n",
    "list_df = class_testing.complete_scrape(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break up df from list returned\n",
    "search_df = list_df[0]\n",
    "amazon_df = list_df[1]\n",
    "revenue_df = list_df[2]\n",
    "product_data_df = list_df[3]\n",
    "etsy_df = list_df[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Fillable Candy Box - In Case of Emergency Brea...</td>\n",
       "      <td>155.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Buttered Popcorn Cotton Candy\\nChocolateStoryb...</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Shimmer Rainbow Unicorn Poop Candy! Happy, Edi...</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Honeycomb candy - Traditional, Cinnamon, or Pe...</td>\n",
       "      <td>18.00 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>candy</th>\n",
       "      <td>Candy Cross - Candy Decoration 25/50/100 Piece...</td>\n",
       "      <td>5.00 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>Vegan Chocolate Assortment Gift Box\\nLMChocola...</td>\n",
       "      <td>16.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>25 truffle wrappers/chocolate paper holders -F...</td>\n",
       "      <td>11.50 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>OMG Toffee, English Toffee, Butter Crunch Dark...</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>Send a letter written in Chocolate! Ghirardell...</td>\n",
       "      <td>28.95 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chocolate</th>\n",
       "      <td>Happy Birthday Chocolates written in Ghirardel...</td>\n",
       "      <td>28.95 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lollipop</th>\n",
       "      <td>Custom Butterfly Hard Candy Sucker Lollipop We...</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lollipop</th>\n",
       "      <td>Galaxy Lollipops, Hard Candy Lollipops, Party ...</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lollipop</th>\n",
       "      <td>Cassidy's Candy Gourmet Lollipops- 12 Pack (Va...</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lollipop</th>\n",
       "      <td>Lollipop Garden - Fat Quarter Bundle - 32 (18\"...</td>\n",
       "      <td>91.00 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lollipop</th>\n",
       "      <td>Birthday Sprinkle Glitter Lollipops, Set of 8 ...</td>\n",
       "      <td>21.99 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bananas</th>\n",
       "      <td>Banana Cat Bed House Cozy Cute Banana Puppy Cu...</td>\n",
       "      <td>29.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bananas</th>\n",
       "      <td>15 Jungle Balloons Rainforest Safari Theme Con...</td>\n",
       "      <td>7.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bananas</th>\n",
       "      <td>Large Vibrating Banana Dildo\\nVEGANTOYS\\n5 out...</td>\n",
       "      <td>32.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bananas</th>\n",
       "      <td>Bananas\\ntheyounglass\\n5 out of 5 stars\\n(342)...</td>\n",
       "      <td>3.50 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bananas</th>\n",
       "      <td>Musa Orinoco Banana Plant\\nTropicalCoast\\n5 ...</td>\n",
       "      <td>25.00 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oranges</th>\n",
       "      <td>Aromatherapy Essential Oils Gift Set Pure Orga...</td>\n",
       "      <td>10.98 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oranges</th>\n",
       "      <td>Solid Walnut Inspiration Tokens Set of 7 // Or...</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oranges</th>\n",
       "      <td>Starship Troopers Inspired - Minimalist Movie ...</td>\n",
       "      <td>24.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oranges</th>\n",
       "      <td>4x4x13mm Square Tube beads -Sea Sediment tube ...</td>\n",
       "      <td>8.00 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oranges</th>\n",
       "      <td>Mens Scrub Hat, Surgical Cap or Chefs Cap with...</td>\n",
       "      <td>15.25 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apples</th>\n",
       "      <td>Apple Bottom - Laser Engraved Herb Grinder\\nGo...</td>\n",
       "      <td>22.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apples</th>\n",
       "      <td>NytStnd TRAY 5 Black- FREE SHIPPING Charging S...</td>\n",
       "      <td>89.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apples</th>\n",
       "      <td>Poison Apples\\nEpiphanyVisionsNVArt\\n$350.00</td>\n",
       "      <td>350.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apples</th>\n",
       "      <td>Nookie Cookie: Feminine bar w/ organic apple ...</td>\n",
       "      <td>7.99 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apples</th>\n",
       "      <td>Apple Crumb Pie Sale...Delicious 9 inch Bakery...</td>\n",
       "      <td>9.99 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tacos</th>\n",
       "      <td>Harem Pants Baby with Tacos Print . Baby Pants...</td>\n",
       "      <td>14.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tacos</th>\n",
       "      <td>Bachelorette Party Shirts, Adios Bitchachos Sh...</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tacos</th>\n",
       "      <td>As for me and my house we will serve tacos woo...</td>\n",
       "      <td>19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tacos</th>\n",
       "      <td>Father Son Matching Taco shirts, Father Daught...</td>\n",
       "      <td>44.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tacos</th>\n",
       "      <td>Eating Tacos for Two Shirt / Pregnancy t-shirt...</td>\n",
       "      <td>15.00 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kebabs</th>\n",
       "      <td>20 Party Favors Candy Kabob Skewers Sticks, Lo...</td>\n",
       "      <td>79.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kebabs</th>\n",
       "      <td>Kebabs Before Abs,funny towel,funny gifts,bant...</td>\n",
       "      <td>32.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kebabs</th>\n",
       "      <td>18 Candy Kabob Individually Bagged Party Favor...</td>\n",
       "      <td>36.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kebabs</th>\n",
       "      <td>Kebabs Not Abs Novelty Funny Amazing Home Deco...</td>\n",
       "      <td>6.52 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kebabs</th>\n",
       "      <td>Faik's Turkish Terracotta \"Testi\" Pot, for uni...</td>\n",
       "      <td>24.99 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandwiches</th>\n",
       "      <td>3 Reusable ETEE Food Wraps | Biodegradable | ...</td>\n",
       "      <td>16.86 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandwiches</th>\n",
       "      <td>Retro Apron Burgers and Sandwiches on Black - ...</td>\n",
       "      <td>28.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandwiches</th>\n",
       "      <td>Walla Walla Onion Plants- Sweet ,Delicious for...</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandwiches</th>\n",
       "      <td>Bulk Reusable Sandwich Bags Zipper Food Pouch ...</td>\n",
       "      <td>40.00 FREE shipping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandwiches</th>\n",
       "      <td>Ornate Wood Toothpicks 360 Count, Toothpicks F...</td>\n",
       "      <td>13.99 FREE shipping</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  product_name  \\\n",
       "search_term                                                      \n",
       "candy        Fillable Candy Box - In Case of Emergency Brea...   \n",
       "candy        Buttered Popcorn Cotton Candy\\nChocolateStoryb...   \n",
       "candy        Shimmer Rainbow Unicorn Poop Candy! Happy, Edi...   \n",
       "candy        Honeycomb candy - Traditional, Cinnamon, or Pe...   \n",
       "candy        Candy Cross - Candy Decoration 25/50/100 Piece...   \n",
       "chocolate    Vegan Chocolate Assortment Gift Box\\nLMChocola...   \n",
       "chocolate    25 truffle wrappers/chocolate paper holders -F...   \n",
       "chocolate    OMG Toffee, English Toffee, Butter Crunch Dark...   \n",
       "chocolate    Send a letter written in Chocolate! Ghirardell...   \n",
       "chocolate    Happy Birthday Chocolates written in Ghirardel...   \n",
       "lollipop     Custom Butterfly Hard Candy Sucker Lollipop We...   \n",
       "lollipop     Galaxy Lollipops, Hard Candy Lollipops, Party ...   \n",
       "lollipop     Cassidy's Candy Gourmet Lollipops- 12 Pack (Va...   \n",
       "lollipop     Lollipop Garden - Fat Quarter Bundle - 32 (18\"...   \n",
       "lollipop     Birthday Sprinkle Glitter Lollipops, Set of 8 ...   \n",
       "bananas      Banana Cat Bed House Cozy Cute Banana Puppy Cu...   \n",
       "bananas      15 Jungle Balloons Rainforest Safari Theme Con...   \n",
       "bananas      Large Vibrating Banana Dildo\\nVEGANTOYS\\n5 out...   \n",
       "bananas      Bananas\\ntheyounglass\\n5 out of 5 stars\\n(342)...   \n",
       "bananas      Musa Orinoco Banana Plant\\nTropicalCoast\\n5 ...   \n",
       "oranges      Aromatherapy Essential Oils Gift Set Pure Orga...   \n",
       "oranges      Solid Walnut Inspiration Tokens Set of 7 // Or...   \n",
       "oranges      Starship Troopers Inspired - Minimalist Movie ...   \n",
       "oranges      4x4x13mm Square Tube beads -Sea Sediment tube ...   \n",
       "oranges      Mens Scrub Hat, Surgical Cap or Chefs Cap with...   \n",
       "apples       Apple Bottom - Laser Engraved Herb Grinder\\nGo...   \n",
       "apples       NytStnd TRAY 5 Black- FREE SHIPPING Charging S...   \n",
       "apples            Poison Apples\\nEpiphanyVisionsNVArt\\n$350.00   \n",
       "apples       Nookie Cookie: Feminine bar w/ organic apple ...   \n",
       "apples       Apple Crumb Pie Sale...Delicious 9 inch Bakery...   \n",
       "tacos        Harem Pants Baby with Tacos Print . Baby Pants...   \n",
       "tacos        Bachelorette Party Shirts, Adios Bitchachos Sh...   \n",
       "tacos        As for me and my house we will serve tacos woo...   \n",
       "tacos        Father Son Matching Taco shirts, Father Daught...   \n",
       "tacos        Eating Tacos for Two Shirt / Pregnancy t-shirt...   \n",
       "kebabs       20 Party Favors Candy Kabob Skewers Sticks, Lo...   \n",
       "kebabs       Kebabs Before Abs,funny towel,funny gifts,bant...   \n",
       "kebabs       18 Candy Kabob Individually Bagged Party Favor...   \n",
       "kebabs       Kebabs Not Abs Novelty Funny Amazing Home Deco...   \n",
       "kebabs       Faik's Turkish Terracotta \"Testi\" Pot, for uni...   \n",
       "sandwiches   3 Reusable ETEE Food Wraps | Biodegradable | ...   \n",
       "sandwiches   Retro Apron Burgers and Sandwiches on Black - ...   \n",
       "sandwiches   Walla Walla Onion Plants- Sweet ,Delicious for...   \n",
       "sandwiches   Bulk Reusable Sandwich Bags Zipper Food Pouch ...   \n",
       "sandwiches   Ornate Wood Toothpicks 360 Count, Toothpicks F...   \n",
       "\n",
       "                   product_price  \n",
       "search_term                       \n",
       "candy                     155.00  \n",
       "candy                       4.95  \n",
       "candy                      16.00  \n",
       "candy        18.00 FREE shipping  \n",
       "candy         5.00 FREE shipping  \n",
       "chocolate                  16.00  \n",
       "chocolate    11.50 FREE shipping  \n",
       "chocolate                   8.00  \n",
       "chocolate    28.95 FREE shipping  \n",
       "chocolate    28.95 FREE shipping  \n",
       "lollipop                   40.00  \n",
       "lollipop                   18.00  \n",
       "lollipop                  10.20   \n",
       "lollipop     91.00 FREE shipping  \n",
       "lollipop     21.99 FREE shipping  \n",
       "bananas                    29.20  \n",
       "bananas                     7.99  \n",
       "bananas                    32.61  \n",
       "bananas       3.50 FREE shipping  \n",
       "bananas      25.00 FREE shipping  \n",
       "oranges      10.98 FREE shipping  \n",
       "oranges                    32.00  \n",
       "oranges                    24.00  \n",
       "oranges       8.00 FREE shipping  \n",
       "oranges      15.25 FREE shipping  \n",
       "apples                     22.00  \n",
       "apples                    89.10   \n",
       "apples                    350.00  \n",
       "apples        7.99 FREE shipping  \n",
       "apples        9.99 FREE shipping  \n",
       "tacos                     14.40   \n",
       "tacos                     11.98   \n",
       "tacos                      19.99  \n",
       "tacos                     44.00   \n",
       "tacos        15.00 FREE shipping  \n",
       "kebabs                     79.00  \n",
       "kebabs                     32.68  \n",
       "kebabs                     36.00  \n",
       "kebabs        6.52 FREE shipping  \n",
       "kebabs       24.99 FREE shipping  \n",
       "sandwiches   16.86 FREE shipping  \n",
       "sandwiches                 28.75  \n",
       "sandwiches                  3.99  \n",
       "sandwiches   40.00 FREE shipping  \n",
       "sandwiches   13.99 FREE shipping  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etsy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = revenue_df.set_index(['search_term'])\n",
    "keywords=['pen']\n",
    "df = pd.DataFrame()\n",
    "for x in keywords:\n",
    "    df = df.append(amazon_df.loc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>Practical Survival Pen Flashlight Cartridges</td>\n",
       "      <td>17.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>Zebra Pen Retractable Ballpoint 12221</td>\n",
       "      <td>5.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>MyLifeUNIT Fineliner Colored Drawing Assorted</td>\n",
       "      <td>11.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>Fineliner Journaling Calendar Coloring Supplies</td>\n",
       "      <td>24.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>Fisher Space AG7 Original Astronaut</td>\n",
       "      <td>6.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>Ballpoint Cambond Stainless Attendant Retractable</td>\n",
       "      <td>12.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>Wordsworth Black Fountain BLACK GOLD</td>\n",
       "      <td>12.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>Zebra Ballpoint Stainless Retractable Pen</td>\n",
       "      <td>7.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>iMeaniy Ballpoint Signature Colleague Executive</td>\n",
       "      <td>13.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pen</th>\n",
       "      <td>Pilot Precise Retractable Ink 5mm 26062</td>\n",
       "      <td>5.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  product_name product_price\n",
       "search_term                                                                 \n",
       "pen               Practical Survival Pen Flashlight Cartridges         17.99\n",
       "pen                      Zebra Pen Retractable Ballpoint 12221          5.63\n",
       "pen              MyLifeUNIT Fineliner Colored Drawing Assorted         11.37\n",
       "pen            Fineliner Journaling Calendar Coloring Supplies         24.99\n",
       "pen                        Fisher Space AG7 Original Astronaut          6.29\n",
       "pen          Ballpoint Cambond Stainless Attendant Retractable         12.19\n",
       "pen                       Wordsworth Black Fountain BLACK GOLD         12.49\n",
       "pen                  Zebra Ballpoint Stainless Retractable Pen          7.67\n",
       "pen            iMeaniy Ballpoint Signature Colleague Executive         13.49\n",
       "pen                    Pilot Precise Retractable Ink 5mm 26062          5.95"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an engine for the chinook.sqlite database\n",
    "engine = create_engine(\"sqlite:///../static/db/top_trends.db\", echo=False)\n",
    "# Declare a Base using `automap_base()`\n",
    "Base = automap_base()\n",
    "# Use the Base class to reflect the database tables\n",
    "Base.prepare(engine, reflect=True)\n",
    "# Base.metadata.create_all(engine)\n",
    "# create conn\n",
    "conn = engine.connect()\n",
    "# To push the objects made and query the server we use a Session object\n",
    "session = Session(bind=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to database\n",
    "try:\n",
    "    search_df.to_sql('search', engine, if_exists='append')\n",
    "    amazon_df.to_sql('amazon', engine, if_exists='append')\n",
    "    revenue_df.to_sql('total_revenue_h10', engine, if_exists='append')\n",
    "    product_data_df.to_sql('monthly_revenue_h10', engine, if_exists='append')\n",
    "    etsy_df.to_sql('etsy', engine, if_exists='append')\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon',\n",
       " 'categories',\n",
       " 'etsy',\n",
       " 'monthly_revenue_h10',\n",
       " 'region',\n",
       " 'search',\n",
       " 'time',\n",
       " 'total_revenue_h10']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base.classes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
